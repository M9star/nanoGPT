{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M9star/nanoGPT/blob/main/tiny_ai_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Tiny AI Chatbot  \n",
        "### A Hands-On Workshop on How AI Learns to Talk\n",
        "\n",
        "![AI Chatbot Banner](https://iili.io/fNdUQ72.png)\n",
        "\n",
        "---\n",
        "\n",
        "##  Introduction\n",
        "\n",
        "Artificial Intelligence (AI) can now **understand and generate human-like language**.  \n",
        "From chatbots and virtual assistants to automated customer support, AI systems are learning how to *talk* and this notebook shows **how that actually works**, step by step.\n",
        "\n",
        "In this notebook, we build a **simple AI chatbot** and train it using real text data. No advanced math or deep AI background is required.\n",
        "\n",
        "For a deeper understanding of how Large Language Models (LLMs) work under the hood, watch this excellent talk by Andrej Karpathy:  \n",
        "https://www.youtube.com/watch?v=bZQun8Y4L2A\n",
        "\n",
        "---\n",
        "\n",
        "##  What You Will Learn\n",
        "\n",
        "By the end of this session, you will understand:\n",
        "\n",
        "- How AI learns language from **text data**\n",
        "- What a **language model** is and how it works\n",
        "- The difference between **raw data** and **trained intelligence**\n",
        "- How chatbots learn to:\n",
        "  - Hold conversations  \n",
        "  - Tell stories  \n",
        "  - Solve basic math problems\n",
        "- How a trained AI model can be used through a **simple chat interface**\n",
        "\n",
        "---\n",
        "\n",
        "##  What We Will Build\n",
        "\n",
        "We will build a **Tiny AI Chatbot** that can:\n",
        "\n",
        "- Chat with users naturally\n",
        "- Tell short stories\n",
        "- Answer math questions\n",
        "- Remember short conversation history\n",
        "\n",
        "---\n",
        "\n",
        "##  Tools & Technologies Used\n",
        "\n",
        "- **Python** – programming language\n",
        "- **Hugging Face Datasets** – for text data\n",
        "- **Transformers (GPT-2)** – the AI language model\n",
        "- **Gradio** – to create a chat interface\n",
        "\n",
        "\n",
        "\n",
        "Let’s begin our journey!"
      ],
      "metadata": {
        "id": "H2E2TZMvh_as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required libraries\n",
        "- **datasets** – to download and manage text datasets\n",
        "- **transformers** – to load and train the GPT-2 language model\n",
        "- **accelerate** – to optimize training performance\n",
        "- **torch** – deep learning framework\n",
        "- **gradio** – to create an interactive chat interface\n"
      ],
      "metadata": {
        "id": "8ak3w8cHfY8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets==2.16.0 transformers==4.57.0 accelerate==1.10.1 torch gradio==5.49.0"
      ],
      "metadata": {
        "id": "qief7JeruqCN",
        "outputId": "222167d4-d34c-491e-9cd3-ca6099a5d0ee",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T01:14:05.807614Z",
          "iopub.execute_input": "2026-01-07T01:14:05.807904Z",
          "iopub.status.idle": "2026-01-07T01:14:33.924447Z",
          "shell.execute_reply.started": "2026-01-07T01:14:05.807874Z",
          "shell.execute_reply": "2026-01-07T01:14:33.923739Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.57.0 at https://files.pythonhosted.org/packages/e5/2b/4d2708ac1ff5cd708b6548f4c5812d0ae40d1c28591c4c1c762b6dbdef2d/transformers-4.57.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.9.0))\nReason for being yanked: Error in the setup causing installation issues\u001b[0m\u001b[33m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nlangchain 0.3.27 requires SQLAlchemy<3,>=1.4, but you have sqlalchemy 1.2.19 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "id": "l_-U9Owyq8jo",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-06T10:49:25.758892Z",
          "iopub.execute_input": "2026-01-06T10:49:25.759403Z",
          "iopub.status.idle": "2026-01-06T10:49:25.763350Z",
          "shell.execute_reply.started": "2026-01-06T10:49:25.759369Z",
          "shell.execute_reply": "2026-01-06T10:49:25.762696Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and view Datasets  \n",
        "**Datasets:**  \n",
        "- **DailyDialog**: https://huggingface.co/datasets/roskoN/dailydialog  \n",
        "- **TinyStories**: https://huggingface.co/datasets/roneneldan/TinyStories  \n",
        "- **AI-MO / NuminaMath-CoT**: https://huggingface.co/datasets/AI-MO/NuminaMath-CoT  "
      ],
      "metadata": {
        "id": "A7IZIGowfcxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Download dialog dataset (without trust_remote_code)\n",
        "chat_ds = load_dataset(\"roskoN/dailydialog\")\n",
        "print(chat_ds)"
      ],
      "metadata": {
        "id": "YLu4HmXGuvEe",
        "outputId": "fff502c7-cfa1-4104-8910-3369c2a84267",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T01:14:48.774957Z",
          "iopub.execute_input": "2026-01-07T01:14:48.775515Z",
          "iopub.status.idle": "2026-01-07T01:14:53.688849Z",
          "shell.execute_reply.started": "2026-01-07T01:14:48.775477Z",
          "shell.execute_reply": "2026-01-07T01:14:53.688243Z"
        },
        "colab": {
          "referenced_widgets": [
            "cc3a670b2bab4543a1a9c69d34368eb3",
            "859a7ac5f3ed4a66933e8185ba281fdd",
            "72a24a3e86fd457e84913a280eee3a66",
            "d5fc18612ebd476ca3f7d4e45f765b52",
            "75f8d213aad541b8b402ead077c397d5",
            "f3a4e6ccc0434c729b3db3506dff8795"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/3.67M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc3a670b2bab4543a1a9c69d34368eb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/340k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "859a7ac5f3ed4a66933e8185ba281fdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/337k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72a24a3e86fd457e84913a280eee3a66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5fc18612ebd476ca3f7d4e45f765b52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75f8d213aad541b8b402ead077c397d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3a4e6ccc0434c729b3db3506dff8795"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'acts', 'emotions', 'utterances'],\n        num_rows: 11118\n    })\n    validation: Dataset({\n        features: ['id', 'acts', 'emotions', 'utterances'],\n        num_rows: 1000\n    })\n    test: Dataset({\n        features: ['id', 'acts', 'emotions', 'utterances'],\n        num_rows: 1000\n    })\n})\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Download story dataset\n",
        "story_ds = load_dataset(\"roneneldan/TinyStories\")\n",
        "\n",
        "print(story_ds)\n",
        "print()\n",
        "story_ds['train'][0][\"text\"]"
      ],
      "metadata": {
        "id": "dyWQYE10p7Nq",
        "outputId": "99368db8-0558-48b8-e4be-72b093e687c8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T01:15:06.075221Z",
          "iopub.execute_input": "2026-01-07T01:15:06.075947Z",
          "iopub.status.idle": "2026-01-07T01:15:19.308116Z",
          "shell.execute_reply.started": "2026-01-07T01:15:06.075918Z",
          "shell.execute_reply": "2026-01-07T01:15:19.307526Z"
        },
        "colab": {
          "referenced_widgets": [
            "a78d285bce024f1e8834c75bb5b09e90",
            "7756c41696fa4199b2194eddf2a1db98",
            "0757a84cd06b4607959d68bd854d8399",
            "86cdcf731f7341b493c71976fcfd82bd",
            "36293c45aecc4cbc869a8ef4fb3b2f72",
            "6d4eddeb72f9477384cf07e02a9e3518",
            "f21a6df07bcf43c59c9876fb61c281d7",
            "9a0bce24f588418c99b7076cd47a2b67"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a78d285bce024f1e8834c75bb5b09e90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/249M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7756c41696fa4199b2194eddf2a1db98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0757a84cd06b4607959d68bd854d8399"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/246M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86cdcf731f7341b493c71976fcfd82bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36293c45aecc4cbc869a8ef4fb3b2f72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/9.99M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d4eddeb72f9477384cf07e02a9e3518"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f21a6df07bcf43c59c9876fb61c281d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a0bce24f588418c99b7076cd47a2b67"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 2119719\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 21990\n    })\n})\n\n",
          "output_type": "stream"
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Download math dataset\n",
        "math_ds = load_dataset(\"AI-MO/NuminaMath-CoT\")\n",
        "\n",
        "print(math_ds)\n",
        "print()\n",
        "math_ds['train'][0]"
      ],
      "metadata": {
        "id": "Zq1f5l4PyjfY",
        "outputId": "ceea8a86-443b-4fc7-f788-9e99bdca9185",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T01:15:41.000404Z",
          "iopub.execute_input": "2026-01-07T01:15:41.000720Z",
          "iopub.status.idle": "2026-01-07T01:15:54.568036Z",
          "shell.execute_reply.started": "2026-01-07T01:15:41.000694Z",
          "shell.execute_reply": "2026-01-07T01:15:54.567484Z"
        },
        "colab": {
          "referenced_widgets": [
            "a99bf0b2f0544b5a8357f0b8561fd4bb",
            "a2dceebff24142858c30594ef83a6062",
            "38e174b6b05a4770ab32a2b4f8a1c442",
            "245df642840847d295bec37104e7d344",
            "424f62cd7f9e42d78e8fe3ac9bfcd59a",
            "9ca9083941524efca9f4f9ad1bb7c3ac",
            "70b64ca6714e4eb49f18b7d18faa5be4",
            "ab5a8ac425a94376b051d05a1abced21",
            "d80fa376c7db48d6b6ae8af300c571da"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a99bf0b2f0544b5a8357f0b8561fd4bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/247M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2dceebff24142858c30594ef83a6062"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/247M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38e174b6b05a4770ab32a2b4f8a1c442"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/247M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "245df642840847d295bec37104e7d344"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/247M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "424f62cd7f9e42d78e8fe3ac9bfcd59a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/247M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ca9083941524efca9f4f9ad1bb7c3ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/166k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70b64ca6714e4eb49f18b7d18faa5be4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/859494 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab5a8ac425a94376b051d05a1abced21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d80fa376c7db48d6b6ae8af300c571da"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['source', 'problem', 'solution', 'messages'],\n        num_rows: 859494\n    })\n    test: Dataset({\n        features: ['source', 'problem', 'solution', 'messages'],\n        num_rows: 100\n    })\n})\n\n",
          "output_type": "stream"
        },
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'source': 'synthetic_math',\n 'problem': 'Consider the terms of an arithmetic sequence: $-\\\\frac{1}{3}, y+2, 4y, \\\\ldots$. Solve for $y$.',\n 'solution': 'For an arithmetic sequence, the difference between consecutive terms must be equal. Therefore, we can set up the following equations based on the sequence given:\\n\\\\[ (y + 2) - \\\\left(-\\\\frac{1}{3}\\\\right) = 4y - (y+2) \\\\]\\n\\nSimplify and solve these equations:\\n\\\\[ y + 2 + \\\\frac{1}{3} = 4y - y - 2 \\\\]\\n\\\\[ y + \\\\frac{7}{3} = 3y - 2 \\\\]\\n\\\\[ \\\\frac{7}{3} + 2 = 3y - y \\\\]\\n\\\\[ \\\\frac{13}{3} = 2y \\\\]\\n\\\\[ y = \\\\frac{13}{6} \\\\]\\n\\nThus, the value of $y$ that satisfies the given arithmetic sequence is $\\\\boxed{\\\\frac{13}{6}}$.',\n 'messages': [{'content': 'Consider the terms of an arithmetic sequence: $-\\\\frac{1}{3}, y+2, 4y, \\\\ldots$. Solve for $y$.',\n   'role': 'user'},\n  {'content': 'For an arithmetic sequence, the difference between consecutive terms must be equal. Therefore, we can set up the following equations based on the sequence given:\\n\\\\[ (y + 2) - \\\\left(-\\\\frac{1}{3}\\\\right) = 4y - (y+2) \\\\]\\n\\nSimplify and solve these equations:\\n\\\\[ y + 2 + \\\\frac{1}{3} = 4y - y - 2 \\\\]\\n\\\\[ y + \\\\frac{7}{3} = 3y - 2 \\\\]\\n\\\\[ \\\\frac{7}{3} + 2 = 3y - y \\\\]\\n\\\\[ \\\\frac{13}{3} = 2y \\\\]\\n\\\\[ y = \\\\frac{13}{6} \\\\]\\n\\nThus, the value of $y$ that satisfies the given arithmetic sequence is $\\\\boxed{\\\\frac{13}{6}}$.',\n   'role': 'assistant'}]}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Chat Dataset\n",
        "\n",
        "The chat dataset is enhanced by injecting stories and math problems into conversations.\n",
        "\n",
        "### Key Concepts\n",
        "- Alternating **User** and **Bot** messages\n",
        "- Random insertion of:\n",
        "  - Story prompts and responses\n",
        "  - Math problems and solutions\n",
        "- **END_TOKEN** added after each bot response\n",
        "- All prepared datasets are combined into a single training dataset."
      ],
      "metadata": {
        "id": "gz5m2Dp7l4ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "import random\n",
        "import re\n",
        "\n",
        "# Config\n",
        "# ----------------------------\n",
        "CHAT_SAMPLES  = 2000\n",
        "STORY_SAMPLES = 2000\n",
        "MATH_SAMPLES  = 2000\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "STORY_INSERT_PROB = 0.3\n",
        "MATH_INSERT_PROB = 0.3\n",
        "\n",
        "END_TOKEN = \"\\n\"\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "STORY_PROMPTS = [\n",
        "    \"Write a story.\",\n",
        "    \"Tell a story.\",\n",
        "    \"Create a story.\",\n",
        "    \"Write a short story.\",\n",
        "    \"Make up a story.\",\n",
        "    \"Share a story.\",\n",
        "    \"Invent a story.\",\n",
        "    \"Compose a story.\",\n",
        "    \"Write a narrative.\",\n",
        "    \"Create a narrative.\",\n",
        "    \"Tell a tale.\",\n",
        "    \"Write a fictional story.\",\n",
        "    \"Create a short narrative.\",\n",
        "    \"Tell an original story.\",\n",
        "    \"Write an imaginative story.\",\n",
        "    \"Compose a short tale.\",\n",
        "    \"Create an original story.\",\n",
        "    \"Write a creative story.\",\n",
        "    \"Tell a short tale.\",\n",
        "    \"Make up a short story.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Math LaTeX Normalizer (Gradio-safe)\n",
        "# -------------------------------------------------\n",
        "def normalize_math_tex(text: str) -> str:\n",
        "    # Convert \\[ ... \\] → $$ ... $$\n",
        "    text = re.sub(r\"\\\\\\[(.*?)\\\\\\]\", r\"$$\\1$$\", text, flags=re.DOTALL)\n",
        "\n",
        "    # Convert single $...$ → $$...$$ (ignore existing $$)\n",
        "    text = re.sub(\n",
        "        r\"(?<!\\$)\\$(?!\\$)(.+?)(?<!\\$)\\$(?!\\$)\",\n",
        "        r\"$$\\1$$\",\n",
        "        text,\n",
        "        flags=re.DOTALL,\n",
        "    )\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# Load Story Dataset\n",
        "# -------------------------------------------------\n",
        "story_raw_ds = load_dataset(\n",
        "    \"roneneldan/TinyStories\",\n",
        "    split=f\"train[:{STORY_SAMPLES}]\"\n",
        ")\n",
        "\n",
        "stories = [\n",
        "    s[\"text\"].replace(\"\\n\\n\", \" \").strip()\n",
        "    for s in story_raw_ds\n",
        "]\n",
        "\n",
        "\n",
        "# Load Math Dataset\n",
        "# -------------------------------------------------\n",
        "math_raw_ds = load_dataset(\n",
        "    \"AI-MO/NuminaMath-CoT\",\n",
        "    split=f\"train[:{MATH_SAMPLES}]\"\n",
        ")\n",
        "\n",
        "math_problems = [\n",
        "    (\n",
        "        normalize_math_tex(m[\"problem\"]),\n",
        "        normalize_math_tex(m[\"solution\"])\n",
        "    )\n",
        "    for m in math_raw_ds\n",
        "]\n",
        "\n",
        "\n",
        "# Chat Formatter (Story + Math Injection)\n",
        "# -------------------------------------------------\n",
        "def format_chat(example):\n",
        "    text = \"\"\n",
        "    utterances = example[\"utterances\"]\n",
        "\n",
        "    insert_story = random.random() < STORY_INSERT_PROB\n",
        "    insert_math = random.random() < MATH_INSERT_PROB\n",
        "\n",
        "    insert_idx = random.randrange(0, len(utterances), 2)\n",
        "\n",
        "    for i, sentence in enumerate(utterances):\n",
        "        if i % 2 == 0:\n",
        "            text += f\"User: {sentence}\\n\"\n",
        "        else:\n",
        "            text += f\"Bot: {sentence} {END_TOKEN}\\n\"\n",
        "\n",
        "        if i == insert_idx and i % 2 == 0:\n",
        "            if insert_story:\n",
        "                prompt = random.choice(STORY_PROMPTS)\n",
        "                story = random.choice(stories)\n",
        "                text += f\"User: {prompt}\\n\"\n",
        "                text += f\"Bot: {story} {END_TOKEN}\\n\"\n",
        "\n",
        "            ## Uncomment this if you want to train with math dataset\n",
        "            # elif insert_math:\n",
        "            #     problem, solution = random.choice(math_problems)\n",
        "            #     text += f\"User: {problem}\\n\"\n",
        "            #     text += f\"Bot: {solution} {END_TOKEN}\\n\"\n",
        "\n",
        "    return {\"text\": text.strip()}\n",
        "\n",
        "\n",
        "# Story-only Formatter\n",
        "# -------------------------------------------------\n",
        "def format_story(example):\n",
        "    prompt = random.choice(STORY_PROMPTS)\n",
        "    story = example[\"text\"].strip()  #.replace(\"\\n\\n\", \" \")\n",
        "    return {\n",
        "        \"text\": f\"User: {prompt}\\nBot: {story} {END_TOKEN}\"\n",
        "    }\n",
        "\n",
        "\n",
        "# Math-only Formatter\n",
        "# -------------------------------------------------\n",
        "def format_math(example):\n",
        "    return {\n",
        "        \"text\": (\n",
        "            f\"User: {normalize_math_tex(example['problem'])}\\n\"\n",
        "            f\"Bot: {normalize_math_tex(example['solution'])} {END_TOKEN}\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "\n",
        "# Load & Prepare Chat Dataset\n",
        "# -------------------------------------------------\n",
        "chat_ds = load_dataset(\n",
        "    \"roskoN/dailydialog\",\n",
        "    split=f\"train[:{CHAT_SAMPLES}]\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "chat_ds = chat_ds.map(\n",
        "    format_chat,\n",
        "    remove_columns=chat_ds.column_names\n",
        ")\n",
        "\n",
        "\n",
        "# Prepare Story-only Dataset\n",
        "# -------------------------------------------------\n",
        "story_ds = story_raw_ds.map(\n",
        "    format_story,\n",
        "    remove_columns=story_raw_ds.column_names\n",
        ")\n",
        "\n",
        "\n",
        "# Prepare Math-only Dataset\n",
        "# -------------------------------------------------\n",
        "math_ds = math_raw_ds.map(\n",
        "    format_math,\n",
        "    remove_columns=math_raw_ds.column_names\n",
        ")\n",
        "\n",
        "\n",
        "# Merge + Shuffle\n",
        "# -------------------------------------------------\n",
        "dataset = concatenate_datasets([chat_ds, story_ds])           # Train with Chat + Story dataset\n",
        "#dataset = concatenate_datasets([chat_ds, story_ds, math_ds])  # Train with Chat + Story + Math dataset\n",
        "\n",
        "dataset = dataset.shuffle(seed=SEED)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Sanity Check\n",
        "print(dataset)\n",
        "for i in range(15):\n",
        "    print(\"\\n--- SAMPLE ---\\n\")\n",
        "    print(dataset[i][\"text\"])"
      ],
      "metadata": {
        "id": "bm-Q0830l20w",
        "outputId": "d9ed6d1b-feaa-4c4f-f281-abeee91f1026",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T01:49:09.936846Z",
          "iopub.execute_input": "2026-01-07T01:49:09.937658Z",
          "iopub.status.idle": "2026-01-07T01:49:13.398337Z",
          "shell.execute_reply.started": "2026-01-07T01:49:09.937630Z",
          "shell.execute_reply": "2026-01-07T01:49:13.397618Z"
        },
        "colab": {
          "referenced_widgets": [
            "d081a70aab0a4d7dba017dea8feb5449",
            "af2c481e3d03476996c9718456d33b72",
            "cd5cc682b9974b9c86fe579703ae0d9c"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d081a70aab0a4d7dba017dea8feb5449"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af2c481e3d03476996c9718456d33b72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd5cc682b9974b9c86fe579703ae0d9c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset({\n    features: ['text'],\n    num_rows: 4000\n})\n\n--- SAMPLE ---\n\nUser: That's good to know . We'll watch for crazy drivers .\nBot: Hey , guys . Ready for a pick-up game of touch football ? \n\nUser: Football ? Sounds dangerous . Maybe I'll just be a cheerleader .\nBot: C'mon , Yi-jun . It's lots of fun . You've seen football played on TV . \n\nUser: Uh , yeah . And I've seen guys in pads tackled . We have no pads . I don't want to be tackled .\nBot: There's barely any contact in touch football . People just tag you to stop the play . \n\nUser: OK , but they better tag lightly !\n\n--- SAMPLE ---\n\nUser: Do you speak only English in the class , or does your teacher explain everything to you in Spanish ?\nBot: Oh , we never speak Spanish in class ! Miss.Parker speaks to us only in English . \n\nUser: I suppose she's right.Does she speak English very slowly ?\nBot: Not always.Sometimes we don't understand her.Then she has to repeat what she said . \n\nUser: It must be interesting to study English .\nBot: Mario and I are the best students in the class.Miss.Parker says that my pronunciation is very good . \n\nUser: Is English pronunciation easy or difficult ?\nBot: Sometimes it's difficult.Some words look different from the way one pronounces them .\n\n--- SAMPLE ---\n\nUser: How's your new car ?\nBot: Perfect.Couldn ' t be better ! \n\nUser: You made a good choice , then ?\nBot: It's just what I wanted . \n\nUser: No regrets ?\nBot: I'm really pleased with it.Thanks . \n\nUser: I'm glad you're so happy .\nBot: It's super .\n\n--- SAMPLE ---\n\nUser: Share a story.\nBot: Once there was a little girl called Mia who loved to jump. Everywhere she went, she jumped. When walking to school, she would jump on the sidewalk. At the park, she would jump into the sandbox.\n\nOne day Mia was at the supermarket and she saw something unusual. She saw a lawyer. Mia had never seen a lawyer before so it made her very curious. She wanted to know what a lawyer did and why he was so dressed up. So, Mia jumped right up to the lawyer and asked him.\n\nThe lawyer was very confused. He had never seen a little girl so eager to talk to him. He tried to explain but Mia kept on jumping and interrupting.\n\nSoon enough the store manager got involved. He explained to Mia that it was not appropriate to engage with strangers and it was wrong to interrupt people when they were talking.\n\nMia was very sorry for her behaviour and decided to never do something like this again. She had learned her lesson that it is important to be respectful to all strangers. \n\n\n--- SAMPLE ---\n\nUser: Create a short narrative.\nBot: Tom is a dog who likes to play with his bone. He has a big bone that he found in the park. He thinks it is the best bone ever. He carries it everywhere and chews on it all day.\n\nOne day, Tom sees a new dog in the park. The new dog is big and fluffy and has a shiny collar. Tom thinks the new dog is very pretty. He wants to be friends with her. He runs to her and says, \"Hi, I am Tom. Do you want to play with me and my bone?\"\n\nThe new dog looks at Tom and his bone. She sniffs them and says, \"No, thank you. Your bone is old and dirty. And you are small and ordinary. I only play with dogs who have new and shiny bones. And who are big and special. Go away, Tom. You are boring.\"\n\nTom feels sad and hurt. He does not understand why the new dog is so mean. He likes his bone and he thinks he is a good dog. He does not want to go away. He wants to play and have fun.\n\nBut the new dog ignores him. She turns her back on him and walks away. She does not care about Tom or his feelings. She only cares about herself and her looks.\n\nTom decides that the new dog is not a nice dog. He does not want to be friends with her anymore. He goes back to his bone and chews on it happily. He thinks his bone is the best bone ever. And he is a good dog. He hopes to find other dogs who are kind and friendly. And who like to play with him and his bone. \n\n\n--- SAMPLE ---\n\nUser: Please excuse me , but I really have to be going .\nBot: Yes , of course . It was nice to see you . \n\nUser: It was nice to see you , too . And please give my regards to Mrs.Robbins .\n\n--- SAMPLE ---\n\nUser: I'm feeling hungry .\nBot: Isn't it time for dinner ? \n\nUser: Yes , let's get something to eat .\nBot: How about Mac Donald's ?\n\n--- SAMPLE ---\n\nUser: Write a story.\nBot: Tim and Jen are twins. They like to play with cars and trucks. They have many cars and trucks in their room. They make noises like vroom and beep.\n\nOne day, mom says, \"We are going to the park. Get ready.\" Tim and Jen are happy. They love the park. They want to bring their cars and trucks. They put some in a bag.\n\nMom takes them to the car. She says, \"I have a surprise for you. We have a new driver today. His name is Bob. He is very nice. Say hello to Bob.\"\n\nTim and Jen look at Bob. He is sitting in the front. He has a big nose, a big beard, and a big hat. He smiles at them. He says, \"Hello, Tim and Jen. I am Bob. I am your driver today. Are you ready to go to the park?\"\n\nTim and Jen think Bob is ugly. They do not like his nose, his beard, and his hat. They do not say hello. They hide behind mom. They whisper, \"Mom, we do not like Bob. He is ugly. Can we have another driver?\"\n\nMom says, \"Shh, Tim and Jen. That is not nice. Bob is not ugly. He is different. He is a good driver. He will take us to the park safely. You should be polite and friendly. Say hello to Bob.\"\n\nTim and Jen feel bad. They do not want to be rude. They peek at Bob. He still smiles at them. He says, \"Do you like cars and trucks? I see you have some in your bag. I like cars and trucks too. I have some in my hat. Do you want to see?\"\n\nTim and Jen are curious. They like cars and trucks. They nod. Bob takes off his hat. He shows them his cars and trucks. They are small and colorful. He says, \"These are my cars and trucks. I bring them with me everywhere. They are my friends. Do you want to play with them?\"\n\nTim and Jen are surprised. They think Bob is funny. They like his cars and trucks. They say, \"Yes, please. Can we play with them?\" They take some from his hat. They make noises like vroom and beep.\n\nBob says, \"Sure, you can play with them. But be careful. Do not lose them. They are very special to me.\" He puts his hat back on. He says, \"Now, let's go to the park. Are you ready?\"\n\nTim and Jen say, \"Yes, we are ready. Thank you, Bob. You are a nice driver. And you are not ugly. You are different. And we like different.\" They smile at Bob. They hold their cars and trucks. They are happy.\n\nMom says, \"That's good, Tim and Jen. I am proud of you. You are learning to be kind and respectful. And you are making a new friend. Bob is a good driver. And he is a good person. And he has a good hat.\" She smiles at Bob. She says, \"Thank you, Bob. You are a great driver. And you have a great hat.\" She buckles up. She says, \"Now, let's go to the park. And have some fun.\" \n\n\n--- SAMPLE ---\n\nUser: Write an imaginative story.\nBot: Once upon a time there was a little girl. Her name was Lucy. She was three years old.\n\nEvery morning, Lucy's mommy would raise her up and give her a big smile. Then she'd give her some yummy breakfast. Sometimes it was pancakes with lots of sugar. Lucy's favorite!\n\nAfter breakfast, Lucy would go outside and play. She was very impatient, always running and jumping around. She wanted to do more and more and more.\n\nOne day, Lucy saw her mommy raising a kite in the sky. It flew so high and far away. Lucy was so excited! She wanted to go higher and higher too. So her mommy said, \"Why don't you raise your arms and see how high you can fly?\"\n\nSo Lucy raised her arms and shouted, \"Higher and higher!\" Lucy was amazed at how high she could fly. She smiled and laughed with joy.\n\nLucy loved to raise things and see how far she could go. It made her happy. Sugar and patience had nothing on her energy! \n\n\n--- SAMPLE ---\n\nUser: Will you have time this Saturday evening to go to the cinema with me ?\nBot: Saturday ? Oh , I have a previous engagement . \n\nUser: What a pity ! Then how about Sunday ?\nBot: I've already got plans , too . \n\nUser: Then when do you think we could go out together ?\nBot: How about Friday ? Only in that day can I arrange it . \n\nUser: OK . Any time on Friday will be OK with me .\nBot: That's settled then . I'll contact you later .\n\n--- SAMPLE ---\n\nUser: Write a narrative.\nBot: Once, there was a lucky lady. She was walking through the forest when she heard a loud sound. She came to a clearing and saw a bush shaking. She walked closer and saw a baby bird in the bush, its wings had broken!\n\nRight away, the lucky lady knew what to do. She carefully picked up the baby bird, and she took it home with her. There, she fed it and gave it a warm, cozy bed.\n\nSoon, the baby bird was feeling much better. Its wings were healed and it was able to fly! The lucky lady opened her window and the bird flew away.\n\nThe lady felt so happy and lucky. She knew she had saved the bird's life. \n\n\n--- SAMPLE ---\n\nUser: Share a story.\nBot: Once upon a time, in a busy little town, there was a dog named Rhythm. Rhythm was a very happy dog. He loved to play and dance all day long. His favorite thing to do was to tap his feet to the sound of the music playing in the town square.\n\nOne day, while Rhythm was dancing, he saw a little bird stuck in a tree. The bird was scared and couldn't fly away. Rhythm wanted to help, so he barked very loud to call his friend, Kitty the cat.\n\nKitty came running and asked, \"What's wrong, Rhythm?\" Rhythm pointed to the bird and said, \"We need to rescue the bird. Can you help?\" Kitty nodded and climbed up the tree. She carefully picked up the bird and brought it down to the ground.\n\nThe little bird was so happy to be free. It flapped its wings and sang a beautiful song. Rhythm and Kitty danced to the music, and they all became the best of friends. From that day on, they played and danced together every day, making the busy little town even happier. \n\n\n--- SAMPLE ---\n\nUser: Create a story.\nBot: Jack and Jill were playing chase in the hall. Jack was running and jumping and having lots of fun. Jill got a bit careless and tipped over a flower pot. It made a crash and a mess!\n\nSuddenly, Jill felt scared and started to cry. Jack stopped playing and went to her. He said, â€œItâ€™s ok Jill, letâ€™s clean it up.â€ He helped her clean the mess and then gave her a hug.\n\nJill felt better. She accepted Jackâ€™s help and the two of them were soon playing again. They were even more careful with the flower pots, so no more accidents happened.\n\nJack and Jill were best friends now and they played together in the hall every day. They accepted each otherâ€™s mistakes, laughed a lot and never got careless again. \n\n\n--- SAMPLE ---\n\nUser: Write a story.\nBot: Sally was a three year old girl. She loved to play outside and explore. One day she noticed a wink. It was a frog! Sally was so excited she just had to get it. She ran after the frog. She got close but the frog hopped away.\n\nSally ran after the frog for a very long time. She became so hot and wet from running around. But even though she was very tired, Sally still wanted to catch the frog!\n\nFinally, the frog stopped to rest by a big puddle of water. This was Sally's chance to get the frog. She ran fast, her feet splashing the wet puddle. The frog saw her coming and hopped away. Sally kept running until she finally caught the frog. She gave it a big wink and held it in her hands. Sally was so happy she had got the frog. She kissed it and ran back home! \n\n\n--- SAMPLE ---\n\nUser: Create a narrative.\nBot: Once upon a time, there was a little rabbit called Hop. He was very small and helpless. Every day he would look for something to eat in the meadow. \n\nOne day, he came across something colourful and delicious. It was a carrot. When he was about to eat it, a big, strong fox appeared. The fox told Hop to bring him the carrot. Hop felt helpless because he was too small and scared to say no. He handed over the carrot and the fox ran away.\n\nHop was very upset. Although the fox had taken the carrot away, Hop realised that he should help himself. He went and looked for more carrots in the meadow and eventually, he found them. \n\nHop brought the carrots to the animal community and they all ate together. Everyone was happy and grateful to Hop. He had taught them the moral of the story: that itâ€™s good to help others, yet donâ€™t forget to help yourself too. \n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Base Language Model\n",
        "\n",
        "A pre-trained GPT-2 model is used as the foundation.\n",
        "\n",
        "### Why GPT-2?\n",
        "- Already understands basic language patterns\n",
        "- Small enough for fast training\n",
        "\n",
        "The tokenizer and model are configured to support custom tokens."
      ],
      "metadata": {
        "id": "NKv_yT3dfkQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "FQAlXn-NuvXg",
        "outputId": "b2871150-f949-4674-82ea-87fc95f8dfac",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T01:49:30.548349Z",
          "iopub.execute_input": "2026-01-07T01:49:30.548650Z",
          "iopub.status.idle": "2026-01-07T01:49:57.776162Z",
          "shell.execute_reply.started": "2026-01-07T01:49:30.548625Z",
          "shell.execute_reply": "2026-01-07T01:49:57.775428Z"
        },
        "colab": {
          "referenced_widgets": [
            "01071d67bf2a41d5801b005a4ffdbba5",
            "2daac35c84a346219a8ed7be48dde717",
            "0171f09218dc4c81acf546a0b2f22f56",
            "7128fd79563840f9b7bc63a238403bb8",
            "014beb15427d4d4bbc8f7caf16af8afc",
            "611f0c5f5d6e4914b2f93cec99d04159",
            "8599a653344241928390ee9f5e8c82a6"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2026-01-07 01:49:40.850811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767750581.034843      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767750581.083839      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767750581.530726      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767750581.530765      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767750581.530768      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767750581.530770      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01071d67bf2a41d5801b005a4ffdbba5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2daac35c84a346219a8ed7be48dde717"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0171f09218dc4c81acf546a0b2f22f56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7128fd79563840f9b7bc63a238403bb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "014beb15427d4d4bbc8f7caf16af8afc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "611f0c5f5d6e4914b2f93cec99d04159"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8599a653344241928390ee9f5e8c82a6"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Embedding(50257, 768)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "\n",
        "* vocabulary_size = 50257\n",
        "* dimension = 768\n"
      ],
      "metadata": {
        "id": "zIcC4QTjNInm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alternative models\n",
        "\n",
        "You can experiment with alternative models as well. Uncomment the following cell to use alternative model\n",
        "\n",
        "- **microsoft/DialoGPT-small** (GPT-2 based but chat-trained)\n",
        "- **meta-llama/Llama-2-Chat**\n",
        "- **mistralai/Mistral-7B-Instruct**\n",
        "- **Qwen/Qwen-Chat**"
      ],
      "metadata": {
        "id": "u7JwEidANInm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "##  Uncomment this code to use alternative models\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T01:54:45.734253Z",
          "iopub.execute_input": "2026-01-07T01:54:45.735404Z",
          "iopub.status.idle": "2026-01-07T01:54:45.738901Z",
          "shell.execute_reply.started": "2026-01-07T01:54:45.735372Z",
          "shell.execute_reply": "2026-01-07T01:54:45.738079Z"
        },
        "id": "_lG6QeyqNInm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "Text data must be converted into numbers for the model to understand.\n",
        "\n",
        "#### Tokenization Steps\n",
        "- Convert text into token IDs\n",
        "- Truncate or pad sequences to a fixed length\n",
        "- Create labels for supervised learning"
      ],
      "metadata": {
        "id": "RoKe3bsXNInm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    tokens = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "tokenized_ds = dataset.map(tokenize, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "98c7HWWRuvaT",
        "outputId": "6e4c5424-97a0-43cc-d48a-40f064a141e1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T01:56:18.103435Z",
          "iopub.execute_input": "2026-01-07T01:56:18.104295Z",
          "iopub.status.idle": "2026-01-07T01:56:24.856820Z",
          "shell.execute_reply.started": "2026-01-07T01:56:18.104268Z",
          "shell.execute_reply": "2026-01-07T01:56:24.856006Z"
        },
        "colab": {
          "referenced_widgets": [
            "a152973ab6db4a7395e2341f89106d9b"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a152973ab6db4a7395e2341f89106d9b"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens here:\n",
        "\n",
        "Takes raw text: \"Hello, how are you?\"\n",
        "Converts to token IDs: [15496, 11, 703, 389, 345, 30, ...]\n",
        "\n",
        "Parameters explained:\n",
        "\n",
        "\n",
        "* truncation=True - If text is longer than 128 tokens, cut it off\n",
        "* padding=\"max_length\" - If text is shorter than 128 tokens, pad with pad_token (usually 0 or end-of-sequence token)\n",
        "* max_length=128 - Every sequence will be exactly 128 tokens\n"
      ],
      "metadata": {
        "id": "EIMpFblmNInm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training base model on dataset\n",
        "The model is trained using supervised learning.\n",
        "- The AI predicts the next word in a sentence\n",
        "- Predictions are compared with the correct answer\n",
        "- Errors are used to improve the model\n",
        "- Training runs for multiple epochs\n",
        "\n",
        "The process gradually improves the chatbot’s responses.\n",
        "Increase training epoch or add more data to improve chatbot's response"
      ],
      "metadata": {
        "id": "9eyxfF1NfpXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./chatbot\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "tg5uBGfguvdM",
        "outputId": "733eece0-76e6-4100-ef82-1a1033a698f6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T01:57:55.397252Z",
          "iopub.execute_input": "2026-01-07T01:57:55.398033Z",
          "iopub.status.idle": "2026-01-07T02:01:39.115287Z",
          "shell.execute_reply.started": "2026-01-07T01:57:55.398006Z",
          "shell.execute_reply": "2026-01-07T02:01:39.114563Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 03:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.158600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.949900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.890000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.812200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.831400</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=500, training_loss=1.9284068908691405, metrics={'train_runtime': 221.4418, 'train_samples_per_second': 36.127, 'train_steps_per_second': 2.258, 'total_flos': 522584064000000.0, 'train_loss': 1.9284068908691405, 'epoch': 2.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save our trained model for later use\n",
        "\n",
        "This code saves our trained model on disk so we can load and chat with it later."
      ],
      "metadata": {
        "id": "nMt9yWlXUpWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"./my_gpt2_model\"  # any folder you like\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "LG8DhK4MUkno",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T02:01:49.902840Z",
          "iopub.execute_input": "2026-01-07T02:01:49.903130Z",
          "iopub.status.idle": "2026-01-07T02:01:50.892621Z",
          "shell.execute_reply.started": "2026-01-07T02:01:49.903110Z",
          "shell.execute_reply": "2026-01-07T02:01:50.891998Z"
        },
        "outputId": "26354356-b4cc-4ecf-abe7-b012bc5ecdfa"
      },
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "('./my_gpt2_model/tokenizer_config.json',\n './my_gpt2_model/special_tokens_map.json',\n './my_gpt2_model/vocab.json',\n './my_gpt2_model/merges.txt',\n './my_gpt2_model/added_tokens.json')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets chat with our AI\n",
        "\n",
        "#### Interface Features\n",
        "- Text-based chat interaction\n",
        "- Maintains short conversation history\n",
        "- Displays AI-generated responses instantly\n",
        "\n",
        "#### Conversation Memory Handling\n",
        "The chatbot remembers recent messages to maintain context.\n",
        "\n",
        "#### How It Works\n",
        "- Keeps a limited number of past interactions\n",
        "- Formats them into a structured prompt\n",
        "- Feeds the prompt into the model for response generation\n",
        "\n",
        "This makes conversations feel more natural."
      ],
      "metadata": {
        "id": "3BPKDYABfuan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "MAX_HISTORY = 10\n",
        "\n",
        "def build_prompt(message, history):\n",
        "    history = history[-MAX_HISTORY:]\n",
        "\n",
        "    prompt = \"\"\n",
        "    for user, bot in history:\n",
        "        prompt += f\"User: {user}\\nBot: {bot} {END_TOKEN}\\n\"\n",
        "\n",
        "    prompt += f\"User: {message}\\nBot:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def chat(message, history):\n",
        "    history = history[-MAX_HISTORY:]\n",
        "\n",
        "    prompt = build_prompt(message, history)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    end_token_id = tokenizer.encode(END_TOKEN, add_special_tokens=False)[0]\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=350,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        eos_token_id=end_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the last bot reply\n",
        "    bot_reply = decoded.split(\"Bot:\")[-1]\n",
        "    bot_reply = bot_reply.split(END_TOKEN)[0].strip()\n",
        "\n",
        "    return bot_reply\n",
        "\n",
        "\n",
        "ui_interface = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    title=\"Tiny AI Bot\",\n",
        "    description=\"Trained on our dataset\",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "ui_interface.launch()"
      ],
      "metadata": {
        "id": "pfkIzlBihQU8",
        "outputId": "2f99b8c3-674f-4fbc-d26c-a61109026d2d",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-07T02:02:25.074004Z",
          "iopub.execute_input": "2026-01-07T02:02:25.074316Z",
          "iopub.status.idle": "2026-01-07T02:02:30.968661Z",
          "shell.execute_reply.started": "2026-01-07T02:02:25.074293Z",
          "shell.execute_reply": "2026-01-07T02:02:30.967981Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://a975b244ab8857c72a.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div><iframe src=\"https://a975b244ab8857c72a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
          },
          "metadata": {}
        },
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ThkBwOKsNInn"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}